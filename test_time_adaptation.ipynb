{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025dee1b-a9f9-41a6-87cf-169ffddf06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from hydra import initialize_config_dir, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tbfm import film\n",
    "from tbfm import multisession\n",
    "\n",
    "DATA_DIR = \"/home/mmattb/Projects/opto-coproc/data\"\n",
    "sys.path.append(DATA_DIR)\n",
    "# imported from JNE project\n",
    "import dataset\n",
    "meta = dataset.load_meta(DATA_DIR)\n",
    "\n",
    "OUT_DIR = \"data\"  # Local data cache; i.e. not reading from the opto-coproc folder.\n",
    "EMBEDDING_REST_SUBDIR = \"embedding_rest\"\n",
    "\n",
    "conf_dir = Path(\"./conf\").resolve()\n",
    "\n",
    "# Initialize Hydra with the configuration directory\n",
    "with initialize_config_dir(config_dir=str(conf_dir), version_base=None):\n",
    "    # Compose the configuration\n",
    "    cfg = compose(config_name=\"config\")   # i.e. conf/config.yaml\n",
    "\n",
    "DEVICE = \"cuda:0\" #cfg.device\n",
    "WINDOW_SIZE = cfg.data.trial_len\n",
    "NUM_HELD_OUT_SESSIONS = cfg.training.num_held_out_sessions\n",
    "BASE_MODEL_PATH = \"session3.torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83cfa00a-55d2-44b7-ba16-d45b1bf3d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now a stim data loader...\n",
    "d, held_out_session_ids = multisession.load_stim_batched(                                                             \n",
    "    batch_size=7500,                                                               \n",
    "    window_size=WINDOW_SIZE,                                                               \n",
    "    session_subdir=\"torchraw\",                                                     \n",
    "    data_dir=DATA_DIR,\n",
    "    # held_in_session_ids=[\"MonkeyG_20150914_Session1_S1\", \"MonkeyG_20150915_Session2_S1\"],\n",
    "    held_in_session_ids=[\"MonkeyG_20150925_Session2_S1\", \"MonkeyJ_20160630_Session3_S1\", \"MonkeyG_20150917_Session1_M1\"],\n",
    "    # held_in_session_ids=[\"MonkeyG_20150925_Session2_S1\"],\n",
    "    num_held_out_sessions=NUM_HELD_OUT_SESSIONS,                                                      \n",
    ")\n",
    "data_train, data_test = d.train_test_split(5000, test_cut=2500)\n",
    "\n",
    "held_in_session_ids = data_train.session_ids\n",
    "\n",
    "# Gather cached rest embeddings...\n",
    "embeddings_rest = multisession.load_rest_embeddings(held_in_session_ids, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77464fc3-58d1-4bcc-b7db-2b338b4a4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per session batch size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Verify batch sizes...\n",
    "b = next(iter(data_train))\n",
    "k = list(b.keys())\n",
    "k0 = k[0]\n",
    "\n",
    "\n",
    "b = next(iter(data_train))[k0]\n",
    "\n",
    "print(f\"per session batch size: {b[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5850629-43ba-4d67-9fa5-037ab4861ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and fitting normalizers...\n",
      "Building and warm starting AEs...\n",
      "Loading base TBFM from file...\n",
      "BOOM! Dino DNA!\n"
     ]
    }
   ],
   "source": [
    "# Make the model. Note that AEs will still be PCA warm-started, and normalizers too.\n",
    "\n",
    "ms = multisession.build_from_cfg(cfg, data_train, base_model_path=BASE_MODEL_PATH, device=DEVICE)\n",
    "\n",
    "# TODO: Now figure out how we want to do batch size here...  Probably our efficiency experiment is: batch_size=session_count*bsize.\n",
    "# TODO: get optimizer for FiLM parts only  model_optims = multisession.get_optims(cfg, ms). film.inner_update_stopgrad()\n",
    "# TODO: sample efficiency experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9e3c9ab-7939-4480-b4d8-2bafb4c8187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2081, -0.1057,  0.1777, -0.0155],\n",
      "        [ 0.3461, -0.3218,  0.4037,  0.1337],\n",
      "        [-0.3590,  0.3446, -0.1036,  0.0948],\n",
      "        [ 0.4034, -0.2442, -0.1809,  0.3859]], device='cuda:0',\n",
      "       requires_grad=True) ---\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's do a silly validation: load one of the *held in* sessions and use the warm started AE and normalizers.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This should get okay-ish performance?\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m embeddings_stim, results \u001b[38;5;241m=\u001b[39m multisession\u001b[38;5;241m.\u001b[39mtest_time_adaptation(\n\u001b[1;32m      5\u001b[0m     cfg,\n\u001b[1;32m      6\u001b[0m     ms,\n\u001b[1;32m      7\u001b[0m     embeddings_rest,\n\u001b[1;32m      8\u001b[0m     data_train,\n\u001b[1;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     data_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/multisession.py:385\u001b[0m, in \u001b[0;36mtest_time_adaptation\u001b[0;34m(cfg, model, embeddings_rest, data_train, epochs, data_test, lr, weight_decay, grad_clip)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# We materialize the training data set under the presumption it is small and a single batch.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# TODO we should probably enforce that somehow.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m _, data_train \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39miter_loader(\u001b[38;5;28miter\u001b[39m(data_train), data_train, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 385\u001b[0m embeddings_stim \u001b[38;5;241m=\u001b[39m film\u001b[38;5;241m.\u001b[39minner_update_stopgrad(\n\u001b[1;32m    386\u001b[0m     model,\n\u001b[1;32m    387\u001b[0m     data_train,\n\u001b[1;32m    388\u001b[0m     embeddings_rest,\n\u001b[1;32m    389\u001b[0m     inner_steps\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    390\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    391\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    392\u001b[0m     grad_clip\u001b[38;5;241m=\u001b[39mgrad_clip,\n\u001b[1;32m    393\u001b[0m     quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_test:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/film.py:165\u001b[0m, in \u001b[0;36minner_update_stopgrad\u001b[0;34m(model, data_support, embeddings_rest, inner_steps, lr, weight_decay, grad_clip, quiet)\u001b[0m\n\u001b[1;32m    163\u001b[0m optimizer_inner.zero_grad()\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m--> 165\u001b[0m preds = model(\n\u001b[1;32m    166\u001b[0m     data_support,\n\u001b[1;32m    167\u001b[0m     embeddings_rest=embeddings_rest,\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/_multisession_module.py:29\u001b[0m, in \u001b[0;36mTBFMMultisession.forward\u001b[0;34m(self, data, embeddings_rest, embeddings_stim)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Unpack covariates; go for gold.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m covariates \u001b[38;5;241m=\u001b[39m {sid: d[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sid, d \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 29\u001b[0m latent_forecast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     30\u001b[0m     runways_latent,\n\u001b[1;32m     31\u001b[0m     covariates,\n\u001b[1;32m     32\u001b[0m     embeddings_rest\u001b[38;5;241m=\u001b[39membeddings_rest,\n\u001b[1;32m     33\u001b[0m     embeddings_stim\u001b[38;5;241m=\u001b[39membeddings_stim,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m forecast_decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39mdecode(latent_forecast)\n\u001b[1;32m     37\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms\u001b[38;5;241m.\u001b[39minverse(forecast_decoded)\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/tbfm.py:270\u001b[0m, in \u001b[0;36mSessionDispatcherTBFM.__call__\u001b[0;34m(self, runways, covariates, embeddings_rest, embeddings_stim)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         embedding_stim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m instance(\n\u001b[1;32m    271\u001b[0m         runway,\n\u001b[1;32m    272\u001b[0m         _covariates,\n\u001b[1;32m    273\u001b[0m         embedding_rest\u001b[38;5;241m=\u001b[39membedding_rest,\n\u001b[1;32m    274\u001b[0m         embedding_stim\u001b[38;5;241m=\u001b[39membedding_stim,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     y_hats[sid] \u001b[38;5;241m=\u001b[39m y_hat\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hats\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/tbfm.py:148\u001b[0m, in \u001b[0;36mTBFM.forward\u001b[0;34m(self, runway, stiminds, embedding_rest, embedding_stim)\u001b[0m\n\u001b[1;32m    145\u001b[0m x0 \u001b[38;5;241m=\u001b[39m runway[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# bases: (all batch, time, num_bases)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m bases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbases(\n\u001b[1;32m    149\u001b[0m     stiminds, embedding_rest\u001b[38;5;241m=\u001b[39membedding_rest, embedding_stim\u001b[38;5;241m=\u001b[39membedding_stim\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_bases \u001b[38;5;241m=\u001b[39m bases\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# basis_weights: (batch, in_dim * num_bases)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/tbfm_multisession/py-tbfm/tbfm/bases.py:172\u001b[0m, in \u001b[0;36mBases.forward\u001b[0;34m(self, stiminds, embedding_rest, embedding_stim)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# print(\"hidden1\", hidden[0])\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# print(hidden[0])\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# import pdb\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# bases: (batch, time*num_bases)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m bases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(hidden)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's do a silly validation: load one of the *held in* sessions and use the warm started AE and normalizers.\n",
    "# This should get okay-ish performance?\n",
    "\n",
    "embeddings_stim, results = multisession.test_time_adaptation(\n",
    "    cfg,\n",
    "    ms,\n",
    "    embeddings_rest,\n",
    "    data_train,\n",
    "    epochs=1000,\n",
    "    data_test=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42a7f4-6afe-4c38-8889-b300299917d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then: let's try on a held-out session, but full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ecfcd0-1204-4515-915a-c9fc43ebeec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 1.8234667778015137 1.486933708190918 -0.8891817132631937 -0.5328980684280396\n",
      "---- 1000 0.5899742841720581 0.5507943630218506 0.3826422741015752 0.43499526381492615\n",
      "---- 2000 0.581556499004364 0.549232006072998 0.39112886786460876 0.43645283579826355\n",
      "---- 3000 0.578947901725769 0.5422236323356628 0.3938383956750234 0.44358503818511963\n",
      "---- 4000 0.5930135250091553 0.5486187934875488 0.3809671103954315 0.43679529428482056\n",
      "---- 5000 0.572654664516449 0.535224437713623 0.40044142802556354 0.4508146643638611\n",
      "---- 6000 0.5688191652297974 0.5323787927627563 0.40421965221563977 0.45374569296836853\n",
      "---- 7000 0.5617053508758545 0.532183051109314 0.41133420666058856 0.45391416549682617\n",
      "---- 8000 0.5615584850311279 0.5309649705886841 0.4116242031256358 0.455045223236084\n",
      "---- 9000 0.5558925867080688 0.5284721255302429 0.41714655856291455 0.4577196538448334\n",
      "---- 10000 0.5591580271720886 0.5314217805862427 0.4140358219544093 0.45462727546691895\n",
      "---- 11000 0.554527223110199 0.5307040214538574 0.4188096324602763 0.45540130138397217\n",
      "---- 12000 0.5540117621421814 0.5283955335617065 0.4193418025970459 0.4577757716178894\n",
      "---- 13000 0.5536631345748901 0.5274874567985535 0.4197331766287486 0.45871883630752563\n",
      "---- 14000 0.5518299341201782 0.52720707654953 0.4215191255013148 0.4589216113090515\n",
      "---- 15000 0.5519759654998779 0.5281938314437866 0.4214620441198349 0.4579648971557617\n",
      "---- 16000 0.5515201091766357 0.5274359583854675 0.422029048204422 0.4587717056274414\n",
      "---- 17000 0.5525158047676086 0.5279923677444458 0.4209387848774592 0.45814844965934753\n",
      "---- 18000 0.5497698187828064 0.5302045345306396 0.42378327747186023 0.45591434836387634\n",
      "---- 19000 0.5606147050857544 0.5337705016136169 0.4133245050907135 0.4520646929740906\n",
      "---- 20000 0.5471466779708862 0.525699257850647 0.4263859937588374 0.46051451563835144\n",
      "Final: 0.525699257850647 0.46051451563835144\n"
     ]
    }
   ],
   "source": [
    "# Cleared for takeoff...\n",
    "embeddings_stim, results = multisession.train_from_cfg(\n",
    "    cfg,\n",
    "    ms,\n",
    "    data_train,\n",
    "    model_optims,\n",
    "    embeddings_rest,\n",
    "    data_test=data_test,\n",
    "    test_interval=1000,\n",
    "    epochs=20001)\n",
    "\n",
    "# 15k batch size 3000 0.6836317658424378 0.24544973075389862\n",
    "# 10k batch size 3000 0.6723785549402237 0.24294960126280785\n",
    "# 7.5k batch size 3000 0.6306539237499237 0.34320169389247895\n",
    "# 5k batch size \n",
    "\n",
    "# 7.5k no ae coadapt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bebe47-1fed-472d-8bd4-149b09362732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
