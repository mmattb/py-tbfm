{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multisession Training: Linear AE vs Flow AE Comparison\n",
    "\n",
    "This notebook compares the performance of two autoencoder architectures:\n",
    "1. **LinearChannelAE**: Linear tied-weight autoencoder (like PCA)\n",
    "2. **FlowChannelAE**: Normalizing flow autoencoder (invertible, nonlinear)\n",
    "\n",
    "Both are tested on the same multisession neural data.\n",
    "\n",
    "**Logging**: All output is saved to `logs/ae_comparison_TIMESTAMP.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "from hydra import initialize_config_dir, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tbfm import film\n",
    "from tbfm import multisession\n",
    "from tbfm import utils\n",
    "\n",
    "DATA_DIR = \"/home/danmuir/Projects/tbfm_multisession/data\"\n",
    "sys.path.append(DATA_DIR)\n",
    "from tbfm import dataset\n",
    "meta = dataset.load_meta(DATA_DIR)\n",
    "\n",
    "OUT_DIR = \"data\"\n",
    "EMBEDDING_REST_SUBDIR = \"embedding_rest\"\n",
    "\n",
    "conf_dir = Path(\"./conf\").resolve()\n",
    "\n",
    "# Initialize Hydra with the configuration directory\n",
    "with initialize_config_dir(config_dir=str(conf_dir), version_base=None):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "WINDOW_SIZE = cfg.data.trial_len\n",
    "NUM_HELD_OUT_SESSIONS = cfg.training.num_held_out_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:12 - Logging to: logs/ae_comparison_20251027_180112.log\n",
      "2025-10-27 18:01:12 - ================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:12 - Experiment: Linear AE vs Flow AE Comparison\n",
      "2025-10-27 18:01:12 - Device: cuda\n",
      "2025-10-27 18:01:12 - Window size: 184\n"
     ]
    }
   ],
   "source": [
    "class ExperimentLogger:\n",
    "    \"\"\"Logger that writes to both console and file with timing information.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir=\"logs\", experiment_name=\"ae_comparison\"):\n",
    "        # Create logs directory\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create log file with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_file = self.log_dir / f\"{experiment_name}_{timestamp}.log\"\n",
    "        \n",
    "        # Setup logger\n",
    "        self.logger = logging.getLogger(experiment_name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Remove existing handlers\n",
    "        self.logger.handlers = []\n",
    "        \n",
    "        # File handler\n",
    "        fh = logging.FileHandler(self.log_file)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        \n",
    "        # Formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "        \n",
    "        self.logger.addHandler(fh)\n",
    "        self.logger.addHandler(ch)\n",
    "        \n",
    "        # Timing\n",
    "        self.start_time = None\n",
    "        self.phase_start = None\n",
    "        \n",
    "        self.info(f\"Logging to: {self.log_file}\")\n",
    "        self.info(\"=\"*80)\n",
    "    \n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n",
    "    \n",
    "    def start_phase(self, phase_name):\n",
    "        \"\"\"Start timing a phase.\"\"\"\n",
    "        self.phase_start = time.time()\n",
    "        self.info(f\"\\n{'='*80}\")\n",
    "        self.info(f\"Starting: {phase_name}\")\n",
    "        self.info(f\"{'='*80}\")\n",
    "    \n",
    "    def end_phase(self, phase_name):\n",
    "        \"\"\"End timing a phase and report duration.\"\"\"\n",
    "        if self.phase_start is not None:\n",
    "            duration = time.time() - self.phase_start\n",
    "            self.info(f\"\\nCompleted: {phase_name}\")\n",
    "            self.info(f\"Duration: {self.format_duration(duration)}\")\n",
    "            self.info(f\"{'='*80}\")\n",
    "            self.phase_start = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_duration(seconds):\n",
    "        \"\"\"Format seconds as HH:MM:SS.\"\"\"\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        if hours > 0:\n",
    "            return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "        else:\n",
    "            return f\"{minutes:02d}:{secs:02d}\"\n",
    "    \n",
    "    def start_training(self, num_epochs, phase_name=\"Training\"):\n",
    "        \"\"\"Start training with progress tracking.\"\"\"\n",
    "        self.training_start = time.time()\n",
    "        self.num_epochs = num_epochs\n",
    "        self.start_phase(phase_name)\n",
    "    \n",
    "    def log_progress(self, epoch, num_epochs, train_loss, test_loss=None, train_r2=None, test_r2=None):\n",
    "        \"\"\"Log training progress with ETA.\"\"\"\n",
    "        elapsed = time.time() - self.training_start\n",
    "        epochs_done = epoch + 1\n",
    "        time_per_epoch = elapsed / epochs_done\n",
    "        eta_seconds = time_per_epoch * (num_epochs - epochs_done)\n",
    "        \n",
    "        progress_pct = (epochs_done / num_epochs) * 100\n",
    "        \n",
    "        msg = f\"Epoch {epoch}/{num_epochs} ({progress_pct:.1f}%) | \"\n",
    "        msg += f\"Train Loss: {train_loss:.6f}\"\n",
    "        \n",
    "        if test_loss is not None:\n",
    "            msg += f\" | Test Loss: {test_loss:.6f}\"\n",
    "        if train_r2 is not None:\n",
    "            msg += f\" | Train R²: {train_r2:.6f}\"\n",
    "        if test_r2 is not None:\n",
    "            msg += f\" | Test R²: {test_r2:.6f}\"\n",
    "        \n",
    "        msg += f\" | Elapsed: {self.format_duration(elapsed)}\"\n",
    "        msg += f\" | ETA: {self.format_duration(eta_seconds)}\"\n",
    "        \n",
    "        self.info(msg)\n",
    "    \n",
    "    def end_training(self, phase_name=\"Training\"):\n",
    "        \"\"\"End training and report total time.\"\"\"\n",
    "        total_time = time.time() - self.training_start\n",
    "        self.info(f\"\\n{phase_name} completed in {self.format_duration(total_time)}\")\n",
    "        self.end_phase(phase_name)\n",
    "\n",
    "# Initialize logger\n",
    "logger = ExperimentLogger(log_dir=\"logs\", experiment_name=\"ae_comparison\")\n",
    "logger.info(\"Experiment: Linear AE vs Flow AE Comparison\")\n",
    "logger.info(f\"Device: {DEVICE}\")\n",
    "logger.info(f\"Window size: {WINDOW_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:15 - \n",
      "================================================================================\n",
      "2025-10-27 18:01:15 - Starting: Data Loading\n",
      "2025-10-27 18:01:15 - ================================================================================\n",
      "2025-10-27 18:01:16 - Loaded 1 sessions\n",
      "2025-10-27 18:01:16 - Training batch size: 31250\n",
      "2025-10-27 18:01:16 - Sessions: ['MonkeyG_20150925_Session2_S1']\n",
      "2025-10-27 18:01:16 - Train batch shape: torch.Size([5000, 20, 60])\n",
      "2025-10-27 18:01:16 - Test batch shape: torch.Size([2500, 20, 60])\n",
      "2025-10-27 18:01:16 - \n",
      "Completed: Data Loading\n",
      "2025-10-27 18:01:16 - Duration: 00:01\n",
      "2025-10-27 18:01:16 - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.start_phase(\"Data Loading\")\n",
    "\n",
    "# Session selection\n",
    "held_in_session_ids=[\"MonkeyG_20150925_Session2_S1\"]\n",
    "\n",
    "num_sessions = len(held_in_session_ids)\n",
    "MAX_BATCH_SIZE = 62500 // 2\n",
    "batch_size = (MAX_BATCH_SIZE // num_sessions) * num_sessions\n",
    "\n",
    "d, held_out_session_ids = multisession.load_stim_batched(\n",
    "    window_size=WINDOW_SIZE,\n",
    "    session_subdir=\"torchraw\",\n",
    "    data_dir=DATA_DIR,\n",
    "    unpack_stiminds=True,\n",
    "    held_in_session_ids=held_in_session_ids,\n",
    "    batch_size=batch_size,\n",
    "    num_held_out_sessions=NUM_HELD_OUT_SESSIONS,\n",
    ")\n",
    "data_train, data_test = d.train_test_split(5000, test_cut=2500)\n",
    "\n",
    "held_in_session_ids = data_train.session_ids\n",
    "\n",
    "# Load cached rest embeddings\n",
    "embeddings_rest = multisession.load_rest_embeddings(held_in_session_ids, device=DEVICE)\n",
    "\n",
    "logger.info(f\"Loaded {len(held_in_session_ids)} sessions\")\n",
    "logger.info(f\"Training batch size: {batch_size}\")\n",
    "logger.info(f\"Sessions: {held_in_session_ids}\")\n",
    "\n",
    "# Check batch shapes\n",
    "b = next(iter(data_train))\n",
    "k0 = list(b.keys())[0]\n",
    "logger.info(f\"Train batch shape: {b[k0][0].shape}\")\n",
    "\n",
    "b = next(iter(data_test))\n",
    "logger.info(f\"Test batch shape: {b[k0][0].shape}\")\n",
    "\n",
    "logger.end_phase(\"Data Loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:18 - Configuration: epochs=7001, latent_dim=50\n"
     ]
    }
   ],
   "source": [
    "def cfg_base(cfg, dim):\n",
    "    \"\"\"Base configuration for both AE types\"\"\"\n",
    "    cfg.ae.training.coadapt = False\n",
    "    cfg.ae.warm_start_is_identity = True\n",
    "    cfg.tbfm.module.use_film_bases = False\n",
    "    cfg.tbfm.module.num_bases = 12\n",
    "    cfg.tbfm.module.latent_dim = 2\n",
    "    cfg.latent_dim = dim\n",
    "    cfg.training.epochs = 7001\n",
    "    cfg.normalizers.module._target_ = \"tbfm.normalizers.ScalerZscore\"\n",
    "    cfg.tbfm.training.lambda_fro = 0.03\n",
    "    return cfg\n",
    "\n",
    "logger.info(\"Configuration: epochs=7001, latent_dim=50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:20 - Training helper function defined\n"
     ]
    }
   ],
   "source": [
    "# Define training wrapper function that adds logging\n",
    "import builtins\n",
    "\n",
    "original_train = multisession.train_from_cfg\n",
    "\n",
    "def train_with_logging(*args, **kwargs):\n",
    "    \"\"\"Wrapper for train_from_cfg that logs progress to logger.\"\"\"\n",
    "    # Store original print function\n",
    "    original_print = builtins.print\n",
    "    \n",
    "    def logging_print(*print_args, **print_kwargs):\n",
    "        # Capture the print output\n",
    "        msg = ' '.join(map(str, print_args))\n",
    "        \n",
    "        # Check if it's a training progress line\n",
    "        if msg.startswith('----'):\n",
    "            parts = msg.split()\n",
    "            if len(parts) >= 5:\n",
    "                try:\n",
    "                    epoch = int(parts[1])\n",
    "                    train_loss = float(parts[2])\n",
    "                    test_loss = float(parts[3])\n",
    "                    train_r2 = float(parts[4])\n",
    "                    test_r2 = float(parts[5]) if len(parts) > 5 else None\n",
    "                    \n",
    "                    logger.log_progress(epoch, kwargs.get('epochs', 7001), train_loss, test_loss, train_r2, test_r2)\n",
    "                    return\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "        \n",
    "        # For other messages, just log normally\n",
    "        if msg.strip() and not msg.startswith('Building') and not msg.startswith('BOOM'):\n",
    "            logger.info(msg)\n",
    "        original_print(*print_args, **print_kwargs)\n",
    "    \n",
    "    # Replace print temporarily\n",
    "    builtins.print = logging_print\n",
    "    \n",
    "    try:\n",
    "        result = original_train(*args, **kwargs)\n",
    "    finally:\n",
    "        builtins.print = original_print\n",
    "    \n",
    "    return result\n",
    "\n",
    "logger.info(\"Training helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Linear Autoencoder (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 17:13:52 - \n",
      "================================================================================\n",
      "2025-10-27 17:13:52 - Starting: EXPERIMENT 1: Linear Autoencoder (Baseline)\n",
      "2025-10-27 17:13:52 - ================================================================================\n",
      "2025-10-27 17:13:53 - Building Linear AE model...\n",
      "2025-10-27 17:13:53 - Autoencoder type: LinearChannelAE\n",
      "2025-10-27 17:13:53 - Latent dim: 50\n"
     ]
    }
   ],
   "source": [
    "logger.start_phase(\"EXPERIMENT 1: Linear Autoencoder (Baseline)\")\n",
    "\n",
    "# Load config with linear AE\n",
    "with initialize_config_dir(config_dir=str(conf_dir), version_base=None):\n",
    "    cfg_linear = compose(config_name=\"config\")  # Default is linear\n",
    "\n",
    "cfg_linear = cfg_base(cfg_linear, dim=50)\n",
    "\n",
    "# Build model\n",
    "logger.info(\"Building Linear AE model...\")\n",
    "ms_linear = multisession.build_from_cfg(cfg_linear, data_train, device=DEVICE, quiet=True)\n",
    "model_optims_linear = multisession.get_optims(cfg_linear, ms_linear)\n",
    "\n",
    "logger.info(f\"Autoencoder type: {type(ms_linear.ae.instances[held_in_session_ids[0]]).__name__}\")\n",
    "logger.info(f\"Latent dim: {cfg_linear.latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 17:13:53 - \n",
      "================================================================================\n",
      "2025-10-27 17:13:53 - Starting: Linear AE Training\n",
      "2025-10-27 17:13:53 - ================================================================================\n",
      "2025-10-27 17:13:53 - Epoch 0/7001 (0.0%) | Train Loss: 1.187163 | Test Loss: 1.151420 | Train R²: 0.254855 | Test R²: 0.393432 | Elapsed: 00:00 | ETA: 44:35\n",
      "2025-10-27 17:14:36 - Epoch 1000/7001 (14.3%) | Train Loss: 0.658903 | Test Loss: 1.034887 | Train R²: 0.363734 | Test R²: 0.454519 | Elapsed: 00:43 | ETA: 04:18\n",
      "2025-10-27 17:15:22 - Epoch 2000/7001 (28.6%) | Train Loss: 0.618455 | Test Loss: 0.991228 | Train R²: 0.404007 | Test R²: 0.477522 | Elapsed: 01:29 | ETA: 03:42\n",
      "2025-10-27 17:16:13 - Epoch 3000/7001 (42.9%) | Train Loss: 0.589484 | Test Loss: 0.957529 | Train R²: 0.433221 | Test R²: 0.495303 | Elapsed: 02:20 | ETA: 03:06\n",
      "2025-10-27 17:17:03 - Epoch 4000/7001 (57.1%) | Train Loss: 0.567299 | Test Loss: 0.934708 | Train R²: 0.455795 | Test R²: 0.507340 | Elapsed: 03:10 | ETA: 02:22\n",
      "2025-10-27 17:17:53 - Epoch 5000/7001 (71.4%) | Train Loss: 0.533861 | Test Loss: 0.891246 | Train R²: 0.488432 | Test R²: 0.530254 | Elapsed: 04:00 | ETA: 01:36\n",
      "2025-10-27 17:18:43 - Epoch 6000/7001 (85.7%) | Train Loss: 0.502976 | Test Loss: 0.870202 | Train R²: 0.517544 | Test R²: 0.541376 | Elapsed: 04:50 | ETA: 00:48\n",
      "2025-10-27 17:19:33 - Epoch 7000/7001 (100.0%) | Train Loss: 0.483359 | Test Loss: 0.864932 | Train R²: 0.535520 | Test R²: 0.544137 | Elapsed: 05:40 | ETA: 00:00\n",
      "2025-10-27 17:19:33 - Final: 0.8666365742683411 0.5432320237159729\n",
      "2025-10-27 17:19:33 - \n",
      "Linear AE Training completed in 05:40\n",
      "2025-10-27 17:19:33 - \n",
      "Completed: Linear AE Training\n",
      "2025-10-27 17:19:33 - Duration: 05:40\n",
      "2025-10-27 17:19:33 - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 0.8666365742683411 0.5432320237159729\n"
     ]
    }
   ],
   "source": [
    "# Train Linear AE with logging\n",
    "logger.start_training(cfg_linear.training.epochs, \"Linear AE Training\")\n",
    "\n",
    "embeddings_stim_linear, results_linear = train_with_logging(\n",
    "    cfg_linear,\n",
    "    ms_linear,\n",
    "    data_train,\n",
    "    model_optims_linear,\n",
    "    embeddings_rest,\n",
    "    data_test=data_test,\n",
    "    test_interval=1000,\n",
    "    epochs=cfg_linear.training.epochs\n",
    ")\n",
    "\n",
    "logger.end_training(\"Linear AE Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 17:19:33 - \n",
      "================================================================================\n",
      "2025-10-27 17:19:33 - Starting: Linear AE Training\n",
      "2025-10-27 17:19:33 - ================================================================================\n",
      "2025-10-27 17:19:33 - Epoch 0/7001 (0.0%) | Train Loss: 0.484650 | Test Loss: 0.867595 | Train R²: 0.534223 | Test R²: 0.542716 | Elapsed: 00:00 | ETA: 08:51\n",
      "2025-10-27 17:20:23 - Epoch 1000/7001 (14.3%) | Train Loss: 0.480487 | Test Loss: 0.865982 | Train R²: 0.537112 | Test R²: 0.543538 | Elapsed: 00:50 | ETA: 05:03\n",
      "2025-10-27 17:21:13 - Epoch 2000/7001 (28.6%) | Train Loss: 0.502539 | Test Loss: 0.868884 | Train R²: 0.517390 | Test R²: 0.542014 | Elapsed: 01:40 | ETA: 04:11\n",
      "2025-10-27 17:22:03 - Epoch 3000/7001 (42.9%) | Train Loss: 0.504160 | Test Loss: 0.861968 | Train R²: 0.518835 | Test R²: 0.545701 | Elapsed: 02:30 | ETA: 03:20\n",
      "2025-10-27 17:22:53 - Epoch 4000/7001 (57.1%) | Train Loss: 0.508790 | Test Loss: 0.857332 | Train R²: 0.515552 | Test R²: 0.548139 | Elapsed: 03:20 | ETA: 02:30\n",
      "2025-10-27 17:23:43 - Epoch 5000/7001 (71.4%) | Train Loss: 0.502248 | Test Loss: 0.855573 | Train R²: 0.522419 | Test R²: 0.549064 | Elapsed: 04:10 | ETA: 01:40\n",
      "2025-10-27 17:24:33 - Epoch 6000/7001 (85.7%) | Train Loss: 0.504846 | Test Loss: 0.847996 | Train R²: 0.519341 | Test R²: 0.553101 | Elapsed: 05:00 | ETA: 00:50\n",
      "2025-10-27 17:25:24 - Epoch 7000/7001 (100.0%) | Train Loss: 0.496505 | Test Loss: 0.847878 | Train R²: 0.527095 | Test R²: 0.553132 | Elapsed: 05:51 | ETA: 00:00\n",
      "2025-10-27 17:25:24 - Final: 0.84388667345047 0.5552590489387512\n",
      "2025-10-27 17:25:24 - \n",
      "Linear AE Training completed in 05:51\n",
      "2025-10-27 17:25:24 - \n",
      "Completed: Linear AE Training\n",
      "2025-10-27 17:25:24 - Duration: 05:51\n",
      "2025-10-27 17:25:24 - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: 0.84388667345047 0.5552590489387512\n"
     ]
    }
   ],
   "source": [
    "# Custom training loop with logging\n",
    "logger.start_training(cfg_linear.training.epochs, \"Linear AE Training\")\n",
    "\n",
    "# Monkey-patch the train function to add logging\n",
    "import types\n",
    "\n",
    "original_train = multisession.train_from_cfg\n",
    "\n",
    "def train_with_logging(*args, **kwargs):\n",
    "    # Store original print function\n",
    "    import builtins\n",
    "    original_print = builtins.print\n",
    "    \n",
    "    # Track epoch info\n",
    "    epoch_info = {'last_epoch': 0, 'last_train_loss': 0, 'last_test_loss': 0, 'last_train_r2': 0, 'last_test_r2': 0}\n",
    "    \n",
    "    def logging_print(*print_args, **print_kwargs):\n",
    "        # Capture the print output\n",
    "        msg = ' '.join(map(str, print_args))\n",
    "        \n",
    "        # Check if it's a training progress line\n",
    "        if msg.startswith('----'):\n",
    "            parts = msg.split()\n",
    "            if len(parts) >= 5:\n",
    "                try:\n",
    "                    epoch = int(parts[1])\n",
    "                    train_loss = float(parts[2])\n",
    "                    test_loss = float(parts[3])\n",
    "                    train_r2 = float(parts[4])\n",
    "                    test_r2 = float(parts[5]) if len(parts) > 5 else None\n",
    "                    \n",
    "                    logger.log_progress(epoch, kwargs.get('epochs', 7001), train_loss, test_loss, train_r2, test_r2)\n",
    "                    return\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "        \n",
    "        # For other messages, just log normally\n",
    "        if msg.strip() and not msg.startswith('Building') and not msg.startswith('BOOM'):\n",
    "            logger.info(msg)\n",
    "        original_print(*print_args, **print_kwargs)\n",
    "    \n",
    "    # Replace print temporarily\n",
    "    builtins.print = logging_print\n",
    "    \n",
    "    try:\n",
    "        result = original_train(*args, **kwargs)\n",
    "    finally:\n",
    "        builtins.print = original_print\n",
    "    \n",
    "    return result\n",
    "\n",
    "embeddings_stim_linear, results_linear = train_with_logging(\n",
    "    cfg_linear,\n",
    "    ms_linear,\n",
    "    data_train,\n",
    "    model_optims_linear,\n",
    "    embeddings_rest,\n",
    "    data_test=data_test,\n",
    "    test_interval=1000,\n",
    "    epochs=cfg_linear.training.epochs\n",
    ")\n",
    "\n",
    "logger.end_training(\"Linear AE Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 17:45:57 - \n",
      "Cleaning up Linear AE from GPU memory...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         results_linear[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_hat_test\u001b[39m\u001b[38;5;124m'\u001b[39m][key] \u001b[38;5;241m=\u001b[39m results_linear[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_hat_test\u001b[39m\u001b[38;5;124m'\u001b[39m][key]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Move embeddings to CPU\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m embeddings_stim_linear \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(v) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43membeddings_stim_linear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Clear CUDA cache\u001b[39;00m\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Clean up Linear AE from GPU memory\n",
    "logger.info(\"\\nCleaning up Linear AE from GPU memory...\")\n",
    "\n",
    "# Move model to CPU (needed later for latent space analysis)\n",
    "# if ms_linear is not None:\n",
    "#     ms_linear = ms_linear.cpu()\n",
    "\n",
    "# Delete optimizer (not needed anymore)\n",
    "# del model_optims_linear\n",
    "\n",
    "# Move result tensors to CPU\n",
    "for key in results_linear.get('y_test', {}).keys():\n",
    "    if isinstance(results_linear['y_test'][key], (list, tuple)):\n",
    "        results_linear['y_test'][key] = [t.cpu() if torch.is_tensor(t) else t for t in results_linear['y_test'][key]]\n",
    "    elif torch.is_tensor(results_linear['y_test'][key]):\n",
    "        results_linear['y_test'][key] = results_linear['y_test'][key].cpu()\n",
    "        \n",
    "for key in results_linear.get('y_hat_test', {}).keys():\n",
    "    if torch.is_tensor(results_linear['y_hat_test'][key]):\n",
    "        results_linear['y_hat_test'][key] = results_linear['y_hat_test'][key].cpu()\n",
    "\n",
    "# Move embeddings to CPU\n",
    "embeddings_stim_linear = {k: v.cpu() if torch.is_tensor(v) else v for k, v in embeddings_stim_linear.items()}\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "logger.info(f\"GPU memory freed. Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Flow Autoencoder (Invertible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:30 - \n",
      "================================================================================\n",
      "2025-10-27 18:01:30 - Starting: EXPERIMENT 2: Flow Autoencoder (Invertible)\n",
      "2025-10-27 18:01:30 - ================================================================================\n",
      "2025-10-27 18:01:30 - Building Flow AE model...\n",
      "2025-10-27 18:01:30 - Autoencoder type: FlowChannelAE\n",
      "2025-10-27 18:01:30 - Latent dim: 50\n",
      "2025-10-27 18:01:30 - Flow layers: 4\n",
      "2025-10-27 18:01:30 - Hidden dim: 128\n"
     ]
    }
   ],
   "source": [
    "logger.start_phase(\"EXPERIMENT 2: Flow Autoencoder (Invertible)\")\n",
    "\n",
    "# Load config with flow AE\n",
    "with initialize_config_dir(config_dir=str(conf_dir), version_base=None):\n",
    "    cfg_flow = compose(\n",
    "        config_name=\"config\",\n",
    "        overrides=[\"ae=flow\"]  # Switch to flow autoencoder\n",
    "    )\n",
    "\n",
    "cfg_flow = cfg_base(cfg_flow, dim=50)\n",
    "\n",
    "# Configure flow-specific parameters\n",
    "cfg_flow.ae.module.num_flow_layers = 3\n",
    "cfg_flow.ae.module.hidden_dim = 64\n",
    "\n",
    "# Build model\n",
    "logger.info(\"Building Flow AE model...\")\n",
    "ms_flow = multisession.build_from_cfg(cfg_flow, data_train, device=DEVICE, quiet=True)\n",
    "model_optims_flow = multisession.get_optims(cfg_flow, ms_flow)\n",
    "\n",
    "logger.info(f\"Autoencoder type: {type(ms_flow.ae.instances[held_in_session_ids[0]]).__name__}\")\n",
    "logger.info(f\"Latent dim: {cfg_flow.latent_dim}\")\n",
    "logger.info(f\"Flow layers: {cfg_flow.ae.module.num_flow_layers}\")\n",
    "logger.info(f\"Hidden dim: {cfg_flow.ae.module.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:01:33 - \n",
      "Flow AE Reconstruction Test:\n",
      "2025-10-27 18:01:33 -   Max absolute error: 4.93e-05\n",
      "2025-10-27 18:01:33 -   MSE: 3.82e-11\n",
      "2025-10-27 18:01:33 -   ✗ Not perfectly invertible (error = 4.93e-05)\n"
     ]
    }
   ],
   "source": [
    "# Test invertibility for Flow AE\n",
    "test_batch = next(iter(data_train))\n",
    "session_id = held_in_session_ids[0]\n",
    "x_test = test_batch[session_id][0][:10].to(DEVICE)\n",
    "\n",
    "ae_flow = ms_flow.ae.instances[session_id]\n",
    "mask = torch.arange(x_test.shape[-1]).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = ae_flow.encode(x_test, mask)\n",
    "    x_recon = ae_flow.decode(z, mask)\n",
    "\n",
    "error = (x_test - x_recon).abs().max().item()\n",
    "mse = ((x_test - x_recon)**2).mean().item()\n",
    "logger.info(f\"\\nFlow AE Reconstruction Test:\")\n",
    "logger.info(f\"  Max absolute error: {error:.2e}\")\n",
    "logger.info(f\"  MSE: {mse:.2e}\")\n",
    "if error < 1e-5:\n",
    "    logger.info(f\"  ✓ INVERTIBLE (error < 1e-5)\")\n",
    "else:\n",
    "    logger.info(f\"  ✗ Not perfectly invertible (error = {error:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 18:03:34 - \n",
      "================================================================================\n",
      "2025-10-27 18:03:34 - Starting: Flow AE Training\n",
      "2025-10-27 18:03:34 - ================================================================================\n",
      "2025-10-27 18:03:34 - Epoch 0/7001 (0.0%) | Train Loss: 0.953443 | Test Loss: 1.074933 | Train R²: 0.323728 | Test R²: 0.433621 | Elapsed: 00:00 | ETA: 40:19\n",
      "2025-10-27 18:07:44 - Epoch 1000/7001 (14.3%) | Train Loss: 0.622717 | Test Loss: 0.992339 | Train R²: 0.400810 | Test R²: 0.476995 | Elapsed: 04:09 | ETA: 24:58\n",
      "2025-10-27 18:11:52 - Epoch 2000/7001 (28.6%) | Train Loss: 0.589073 | Test Loss: 0.956695 | Train R²: 0.434432 | Test R²: 0.495777 | Elapsed: 08:18 | ETA: 20:45\n"
     ]
    }
   ],
   "source": [
    "# Train Flow AE with logging\n",
    "logger.start_training(cfg_flow.training.epochs, \"Flow AE Training\")\n",
    "\n",
    "embeddings_stim_flow, results_flow = train_with_logging(\n",
    "    cfg_flow,\n",
    "    ms_flow,\n",
    "    data_train,\n",
    "    model_optims_flow,\n",
    "    embeddings_rest,\n",
    "    data_test=data_test,\n",
    "    test_interval=1000,\n",
    "    epochs=cfg_flow.training.epochs\n",
    ")\n",
    "\n",
    "logger.end_training(\"Flow AE Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.start_phase(\"Results Analysis\")\n",
    "\n",
    "# Summary table\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"RESULTS SUMMARY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"{'Method':<20} {'Test R2':<15} {'Test Loss':<15} {'Train R2':<15}\")\n",
    "logger.info(\"-\"*80)\n",
    "\n",
    "linear_test_r2 = results_linear['final_test_r2']\n",
    "linear_test_loss = results_linear['final_test_loss']\n",
    "linear_train_r2 = results_linear['train_r2s'][-1][1]\n",
    "\n",
    "flow_test_r2 = results_flow['final_test_r2']\n",
    "flow_test_loss = results_flow['final_test_loss']\n",
    "flow_train_r2 = results_flow['train_r2s'][-1][1]\n",
    "\n",
    "logger.info(f\"{'Linear AE':<20} {linear_test_r2:<15.6f} {linear_test_loss:<15.6f} {linear_train_r2:<15.6f}\")\n",
    "logger.info(f\"{'Flow AE':<20} {flow_test_r2:<15.6f} {flow_test_loss:<15.6f} {flow_train_r2:<15.6f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "r2_improvement = ((flow_test_r2 - linear_test_r2) / abs(linear_test_r2)) * 100\n",
    "loss_improvement = ((linear_test_loss - flow_test_loss) / linear_test_loss) * 100\n",
    "\n",
    "logger.info(\"-\"*80)\n",
    "logger.info(f\"{'Improvement':<20} {r2_improvement:+.2f}% {'':>6} {loss_improvement:+.2f}%\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Per-session results\n",
    "logger.info(\"\\nPer-session test R2:\")\n",
    "logger.info(f\"{'Session':<40} {'Linear AE':<15} {'Flow AE':<15} {'Δ':<15}\")\n",
    "logger.info(\"-\"*80)\n",
    "for session_id in results_linear['final_test_r2s'].keys():\n",
    "    linear_r2 = results_linear['final_test_r2s'][session_id]\n",
    "    flow_r2 = results_flow['final_test_r2s'][session_id]\n",
    "    delta = flow_r2 - linear_r2\n",
    "    logger.info(f\"{session_id:<40} {linear_r2:<15.6f} {flow_r2:<15.6f} {delta:+.6f}\")\n",
    "\n",
    "logger.end_phase(\"Results Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training loss\n",
    "ax = axes[0, 0]\n",
    "linear_epochs = [x[0] for x in results_linear['train_losses']]\n",
    "linear_losses = [x[1] for x in results_linear['train_losses']]\n",
    "flow_epochs = [x[0] for x in results_flow['train_losses']]\n",
    "flow_losses = [x[1] for x in results_flow['train_losses']]\n",
    "\n",
    "ax.plot(linear_epochs, linear_losses, label='Linear AE', alpha=0.7)\n",
    "ax.plot(flow_epochs, flow_losses, label='Flow AE', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Test loss\n",
    "ax = axes[0, 1]\n",
    "linear_test_epochs = [x[0] for x in results_linear['test_losses']]\n",
    "linear_test_losses = [x[1] for x in results_linear['test_losses']]\n",
    "flow_test_epochs = [x[0] for x in results_flow['test_losses']]\n",
    "flow_test_losses = [x[1] for x in results_flow['test_losses']]\n",
    "\n",
    "ax.plot(linear_test_epochs, linear_test_losses, label='Linear AE', marker='o', alpha=0.7)\n",
    "ax.plot(flow_test_epochs, flow_test_losses, label='Flow AE', marker='s', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Loss')\n",
    "ax.set_title('Test Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Train R2\n",
    "ax = axes[1, 0]\n",
    "linear_r2_epochs = [x[0] for x in results_linear['train_r2s']]\n",
    "linear_r2_values = [x[1] for x in results_linear['train_r2s']]\n",
    "flow_r2_epochs = [x[0] for x in results_flow['train_r2s']]\n",
    "flow_r2_values = [x[1] for x in results_flow['train_r2s']]\n",
    "\n",
    "ax.plot(linear_r2_epochs, linear_r2_values, label='Linear AE', marker='o', alpha=0.7)\n",
    "ax.plot(flow_r2_epochs, flow_r2_values, label='Flow AE', marker='s', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Train R²')\n",
    "ax.set_title('Train R²')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Test R2\n",
    "ax = axes[1, 1]\n",
    "linear_test_r2_epochs = [x[0] for x in results_linear['test_r2s']]\n",
    "linear_test_r2_values = [x[1] for x in results_linear['test_r2s']]\n",
    "flow_test_r2_epochs = [x[0] for x in results_flow['test_r2s']]\n",
    "flow_test_r2_values = [x[1] for x in results_flow['test_r2s']]\n",
    "\n",
    "ax.plot(linear_test_r2_epochs, linear_test_r2_values, label='Linear AE', marker='o', alpha=0.7)\n",
    "ax.plot(flow_test_r2_epochs, flow_test_r2_values, label='Flow AE', marker='s', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test R²')\n",
    "ax.set_title('Test R²')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Generated training curves plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example predictions\n",
    "session_id = held_in_session_ids[0]\n",
    "channel_idx = 30\n",
    "trial_idx = 0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Linear AE\n",
    "ax = axes[0]\n",
    "y_true = results_linear['y_test'][session_id][2][trial_idx, :, channel_idx].cpu()\n",
    "y_pred = results_linear['y_hat_test'][session_id][trial_idx, :, channel_idx].cpu()\n",
    "ax.plot(y_true, label='True', linewidth=2)\n",
    "ax.plot(y_pred, label='Predicted', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Activity')\n",
    "ax.set_title(f'Linear AE - Channel {channel_idx}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Flow AE\n",
    "ax = axes[1]\n",
    "y_true = results_flow['y_test'][session_id][2][trial_idx, :, channel_idx].cpu()\n",
    "y_pred = results_flow['y_hat_test'][session_id][trial_idx, :, channel_idx].cpu()\n",
    "ax.plot(y_true, label='True', linewidth=2)\n",
    "ax.plot(y_pred, label='Predicted', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Activity')\n",
    "ax.set_title(f'Flow AE - Channel {channel_idx}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Generated prediction comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare latent representations\n",
    "session_id = held_in_session_ids[0]\n",
    "test_batch = next(iter(data_test))\n",
    "x = test_batch[session_id][0][:100].to(DEVICE)\n",
    "mask = torch.arange(x.shape[-1]).to(DEVICE)\n",
    "\n",
    "# Get latent representations\n",
    "with torch.no_grad():\n",
    "    z_linear = ms_linear.ae.instances[session_id].encode(x, mask).cpu().numpy()\n",
    "    z_flow = ms_flow.ae.instances[session_id].encode(x, mask).cpu().numpy()\n",
    "\n",
    "logger.info(f\"\\nLatent Space Analysis:\")\n",
    "logger.info(f\"  Linear AE latent shape: {z_linear.shape}\")\n",
    "logger.info(f\"  Flow AE latent shape: {z_flow.shape}\")\n",
    "logger.info(f\"  Linear AE latent std: {z_linear.std(axis=0).mean():.4f}\")\n",
    "logger.info(f\"  Flow AE latent std: {z_flow.std(axis=0).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 2 latent dimensions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear AE\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(z_linear[:, 0], z_linear[:, 1], \n",
    "                     c=range(len(z_linear)), cmap='viridis', alpha=0.6)\n",
    "ax.set_xlabel('Latent Dim 1')\n",
    "ax.set_ylabel('Latent Dim 2')\n",
    "ax.set_title('Linear AE Latent Space')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "\n",
    "# Flow AE\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(z_flow[:, 0], z_flow[:, 1], \n",
    "                     c=range(len(z_flow)), cmap='viridis', alpha=0.6)\n",
    "ax.set_xlabel('Latent Dim 1')\n",
    "ax.set_ylabel('Latent Dim 2')\n",
    "ax.set_title('Flow AE Latent Space')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax, label='Sample Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Generated latent space visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.start_phase(\"Saving Results\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_results = {\n",
    "    'linear': results_linear,\n",
    "    'flow': results_flow,\n",
    "    'config_linear': OmegaConf.to_container(cfg_linear, resolve=True),\n",
    "    'config_flow': OmegaConf.to_container(cfg_flow, resolve=True),\n",
    "    'summary': {\n",
    "        'linear_test_r2': linear_test_r2,\n",
    "        'flow_test_r2': flow_test_r2,\n",
    "        'r2_improvement_pct': r2_improvement,\n",
    "        'linear_test_loss': linear_test_loss,\n",
    "        'flow_test_loss': flow_test_loss,\n",
    "        'loss_improvement_pct': loss_improvement,\n",
    "    },\n",
    "    'log_file': str(logger.log_file),\n",
    "}\n",
    "\n",
    "results_path = 'ae_comparison_results.torch'\n",
    "torch.save(comparison_results, results_path)\n",
    "logger.info(f\"Results saved to {results_path}\")\n",
    "\n",
    "logger.end_phase(\"Saving Results\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "logger.info(f\"Log file: {logger.log_file}\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Invertibility**: Flow AE achieves near-perfect reconstruction (error < 1e-5), while Linear AE has reconstruction loss\n",
    "2. **Performance**: See comparison table above and log file for detailed metrics\n",
    "3. **Training**: Both models converge successfully with time tracking\n",
    "\n",
    "### When to Use Each:\n",
    "- **Linear AE**: Faster, more interpretable (PCA-like), good baseline\n",
    "- **Flow AE**: Invertible, more expressive, handles nonlinear structure\n",
    "\n",
    "### Logs:\n",
    "- All training progress, metrics, and timing information saved to the log file\n",
    "- Check the `logs/` folder for detailed session logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
