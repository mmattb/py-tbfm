{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dbd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tbfm import tbfm, tbfm_phase_invariant\n",
    "from tbfm.bases import FourierBases\n",
    "from tbfm.test import WaveDataGenerator, plot_grid_activity\n",
    "from tbfm.utils import fft_circular_shift_per_basis\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "RUNWAY_LENGTH = 20\n",
    "TRIAL_LENGTH = 184\n",
    "N_DEPTH = 6\n",
    "TRAIN_SET_SIZE = 3000\n",
    "DEPTH_RUNWAY = 5\n",
    "STIM_DIM = N_DEPTH * DEPTH_RUNWAY\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f011668",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_runways, train_y, train_theta, test_runways, test_y, test_theta = torch.load('blah.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de2115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems to work. Let's try the shift invariant TBFM.\n",
    "NUM_BASES = 20\n",
    "LATENT_DIM = 6\n",
    "BASIS_DEPTH = 4\n",
    "\n",
    "num_channels = train_runways.shape[2]\n",
    "model = tbfm_phase_invariant.TBFMPi(num_channels, STIM_DIM, RUNWAY_LENGTH, NUM_BASES,\n",
    "                  TRIAL_LENGTH-RUNWAY_LENGTH,\n",
    "                  batchy=train_y,\n",
    "                  latent_dim=LATENT_DIM,\n",
    "                  basis_depth=BASIS_DEPTH,\n",
    "                  device=DEVICE)\n",
    "optim = model.get_optim(lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4edbdbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.35 GiB. GPU 0 has a total capacity of 7.65 GiB of which 1.22 GiB is free. Process 85104 has 846.00 MiB memory in use. Including non-PyTorch memory, this process has 5.58 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 31.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eidx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     14\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     yhat_train \u001b[38;5;241m=\u001b[39m model(train_runways, train_theta)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Loss will be based on surface activity predictions only.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(yhat_train, y_train) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/py-tbfm/tbfm/tbfm_phase_invariant.py:155\u001b[0m, in \u001b[0;36mTBFMPi.forward\u001b[0;34m(self, runway, stiminds)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_bases \u001b[38;5;241m=\u001b[39m fourier_bases\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# (batch, in_dim, num_bases, time)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m bases \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mspectrum_shift_ifft(fourier_bases, basis_phases, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrial_len)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Multiply basis weights with bases and sum to get predictions.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# bases: (batch, in_dim, num_bases, time)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# basis_weights: (batch, in_dim, num_bases)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Output shape: (batch, in_dim, time)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m preds \u001b[38;5;241m=\u001b[39m (basis_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m bases)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/py-tbfm/tbfm/utils.py:98\u001b[0m, in \u001b[0;36mspectrum_shift_ifft\u001b[0;34m(spectrum, shift, L, preserve_hermitian)\u001b[0m\n\u001b[1;32m     94\u001b[0m     shifted_spectrum \u001b[38;5;241m=\u001b[39m spectrum \u001b[38;5;241m*\u001b[39m phase\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Inverse FFT to get real-valued time signal. IRFFT automatically\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# handles the Hermitian symmetry for real-valued output.\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m shifted_bases \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mirfft(shifted_spectrum, n\u001b[38;5;241m=\u001b[39mL, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, C, N, L)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shifted_bases\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.35 GiB. GPU 0 has a total capacity of 7.65 GiB of which 1.22 GiB is free. Process 85104 has 846.00 MiB memory in use. Including non-PyTorch memory, this process has 5.58 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 31.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20000\n",
    "TEST_EPOCH_INTERVAL = 1000  # We will run against the test set every so often to monitor progress\n",
    "LAMBDA = 0.01\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "# Now z score our y values using the comming means/stdevs from the training set.\n",
    "# Runways are z scored by the model on the fly, and y_hats will be in that z-scored space\n",
    "y_train = model.zscore(train_y)\n",
    "y_test = model.zscore(test_y)\n",
    "start_time = time.time()\n",
    "for eidx in range(NUM_EPOCHS):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    yhat_train = model(train_runways, train_theta)\n",
    "    \n",
    "    # Loss will be based on surface activity predictions only.\n",
    "    loss = nn.MSELoss()(yhat_train, y_train) \n",
    "\n",
    "    l1w = model.get_weighting_reg()\n",
    "    (loss + LAMBDA * l1w).backward()\n",
    "          \n",
    "    losses_train.append((eidx, loss.item()))\n",
    "    optim.step()\n",
    "    \n",
    "    if (eidx % TEST_EPOCH_INTERVAL) == 0:        \n",
    "        with torch.no_grad():\n",
    "            yhat_test = model(test_runways, test_theta)\n",
    "    \n",
    "            loss = nn.MSELoss()(yhat_test, y_test)\n",
    "            losses_test.append((eidx, loss.item()))\n",
    "            \n",
    "        print(f\"epoch: {eidx}, train loss: {losses_train[-1][1]}, test loss: {losses_test[-1][1]}\")\n",
    "\n",
    "print(f\"Training time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f3663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
